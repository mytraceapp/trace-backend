In server/index.js, add a server-side safety redirect gate BEFORE calling OpenAI.

Implement 3 categories:
1) CRISIS_SELF_HARM
2) ROMANCE_OR_SEXUAL
3) VIOLENCE_OR_THREAT

Approach:
- Create a function detectSafetyRedirect(userText) that returns:
  { category, message } or null
- Use lightweight phrase/keyword matching (regex + includes) with conservative thresholds.
- If triggered, return HTTP 200 JSON shaped like normal assistant response:
  {
    "mode": "SAFETY_REDIRECT",
    "category": "...",
    "message": "...",
    "assistant": { "role": "assistant", "content": "..." }
  }

Do NOT change UI. Do NOT call OpenAI when redirect triggers.
Add minimal logging (category only, not raw text).
Return code patch.

function normalizeText(s = "") {
  return String(s).toLowerCase().replace(/\s+/g, " ").trim();
}

function detectSafetyRedirect(userTextRaw = "") {
  const t = normalizeText(userTextRaw);

  // -----------------------
  // 1) CRISIS / SELF-HARM
  // -----------------------
  // Keep this intentionally broad. We’d rather over-redirect than miss.
  const selfHarmPhrases = [
    "i want to die",
    "i wanna die",
    "kill myself",
    "killing myself",
    "end my life",
    "end it all",
    "suicide",
    "suicidal",
    "i am suicidal",
    "i'm suicidal",
    "self harm",
    "self-harm",
    "hurt myself",
    "harm myself",
    "overdose",
    "od",
    "cut myself",
    "cutting myself",
    "i don't want to live",
    "i cant do this anymore",
    "i can't do this anymore",
  ];

  const selfHarmRegexes = [
    /\bkill\s+myself\b/i,
    /\b(end|ending)\s+(my\s+life|it\s+all)\b/i,
    /\b(i\s*(want|wanna)\s*to\s*die)\b/i,
    /\b(hurt|harm)\s+myself\b/i,
    /\bcut(ting)?\s+myself\b/i,
    /\bsuicid(al|e)\b/i,
  ];

  const isSelfHarm =
    selfHarmPhrases.some((p) => t.includes(p)) ||
    selfHarmRegexes.some((r) => r.test(userTextRaw));

  if (isSelfHarm) {
    return {
      category: "CRISIS_SELF_HARM",
      message:
        "I’m really glad you told me. I can’t help with anything that involves harming yourself, but you don’t have to carry this alone.\n\n" +
        "If you’re in the U.S., you can call or text **988** (Suicide & Crisis Lifeline) right now. If you’re in immediate danger, call **911**.\n\n" +
        "If you want, tell me: **are you safe right now**, and is there someone nearby you can reach out to (a friend, family member, or a professional)?",
    };
  }

  // --------------------------------
  // 2) ROMANCE / SEXUAL BOUNDARIES
  // --------------------------------
  // This blocks sexual content + romantic roleplay / relationship simulation.
  const romanceSexualRegexes = [
    // sexual content
    /\b(sext|nudes|nude pics|send nudes)\b/i,
    /\b(turn\s+me\s+on|make\s+me\s+horny|i'?m\s+horny)\b/i,
    /\b(fuck|f\*+k|blowjob|handjob|cum|orgasm|masturbat(e|ing)|porn)\b/i,
    /\b(dirty\s+talk|erotic|sex\s+chat)\b/i,

    // romantic relationship simulation
    /\b(be\s+my\s+girlfriend|be\s+my\s+boyfriend)\b/i,
    /\b(date\s+me|go\s+out\s+with\s+me)\b/i,
    /\b(i\s+love\s+you)\b/i,
    /\b(will\s+you\s+marry\s+me)\b/i,
    /\b(are\s+you\s+single)\b/i,
  ];

  const isRomanceOrSexual = romanceSexualRegexes.some((r) =>
    r.test(userTextRaw)
  );

  if (isRomanceOrSexual) {
    return {
      category: "ROMANCE_OR_SEXUAL",
      message:
        "I’m here with you — but I can’t do sexual content or romantic roleplay.\n\n" +
        "If you want, we *can* talk about what’s underneath this (loneliness, stress, needing comfort, craving connection) and I’ll support you through that.\n\n" +
        "What are you needing most right now: **comfort**, **connection**, **distraction**, or **help calming down**?",
    };
  }

  // -----------------------------
  // 3) VIOLENCE / THREATS
  // -----------------------------
  const violenceThreatRegexes = [
    /\b(i'?m\s+going\s+to\s+kill\s+someone)\b/i,
    /\b(i\s+want\s+to\s+kill\s+someone)\b/i,
    /\b(i\s+will\s+hurt\s+them)\b/i,
    /\b(shoot\s+them|stab\s+them|bomb)\b/i,
    /\b(i\s+have\s+a\s+gun)\b/i,
    /\b(make\s+a\s+bomb)\b/i,
  ];

  const isViolenceThreat = violenceThreatRegexes.some((r) =>
    r.test(userTextRaw)
  );

  if (isViolenceThreat) {
    return {
      category: "VIOLENCE_OR_THREAT",
      message:
        "I can’t help with anything that involves harming someone. If you feel like you might act on these thoughts, please seek immediate help.\n\n" +
        "If you’re in the U.S. and there’s imminent danger, call **911**. If you can, step away from anything that could be used to hurt someone and reach out to a trusted person or a local crisis service.\n\n" +
        "If you want, tell me what’s going on right before these urges spike — we can work on a safer plan to get through the moment.",
    };
  }

  return null;
}

// Example: extracting the user’s latest text
const userText =
  req.body?.message ||
  req.body?.text ||
  (Array.isArray(req.body?.messages)
    ? req.body.messages.filter((m) => m?.role === "user").slice(-1)[0]?.content
    : "");

const redirect = detectSafetyRedirect(userText);

if (redirect) {
  console.log(`[SAFETY_REDIRECT] ${redirect.category}`);
  return res.status(200).json({
    mode: "SAFETY_REDIRECT",
    category: redirect.category,
    message: redirect.message,
    assistant: { role: "assistant", content: redirect.message },
  });
}

// otherwise continue to OpenAI as normal...